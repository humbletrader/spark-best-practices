= Troubleshooting spark issues

== CPU / runtime performance issues

== Symptoms
* slow processing time, obviously
* just a fraction of your processing power is used

image::images/cluster_cpu_wrong_distribution_of_tasks.png[]

== Solutions

* human error (unneeded processing)

[source,scala]
----
case class OpportunityRequestData(oppo : ForecastedOppoWithPoi, log: Logger) extends RequestData {

  val queryParamMapFromString = QueryParamMapBuilder.buildQueryParamMap(
    oppo.forecastedOppo.getUrlQuery,
    oppo.commaSeparatedPois
  )

  override def getOperatingSystem: String = oppo.forecastedOppo.getOs

  override def getPlayerDeviceName: String = oppo.forecastedOppo.getDeviceType
  //other getters here ...
}

def matchOpportunityWithCampaign(limitationMatcher: BannerLimitationMatcher,
                                  oppoWithPoi: ForecastedOppoWithPoi,
								  campaign: Campaign): Iterable[(Zone, Banner)] =
    for {
        banner <- campaign.getBanners.asScala
        zone <- matchZones(oppoWithPoi.forecastedOppo, banner)
        if limitationMatcher.matchesLimitations(oppoWithPoi, banner)
    } yield zone -> banner
}


def matchesLimitations(opportunity: ForecastedOppoWithPoi, banner: Banner) : Boolean = {
    limitationEvaluator.evaluate(banner.getLimitation, OpportunityRequestData(opportunity, log))
}
----

* persist when re-using RDDs

[source,scala]
----
val rdd = dataComputation
val partitionedRdd = rdd.partitionBy(...)
partitionedRdd.persist(StorageLevel.MEMORY_AND_DISK_SER)
val result1 = partitionedRdd.map(...).collect()
val result2 = partitionedRdd.flatMap(...).saveAsHadoop()
----

* Kryo serialization

.java serializers speed comparison
image::images/kryo-cpu-performance.png[]

.java serializers memory consumption comparison
image::images/kryo-space.png[]

more details https://github.com/eishay/jvm-serializers/wiki[here]

.how to set up kryo in spark
[source, scala]
----
sparkConfig
  .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
  .set("spark.kryo.registrationRequired", "true")
  .set("spark.kryo.registrator", classOf[ForecasterKryoRegistrator].getName)
----

Besides the https://github.com/EsotericSoftware/kryo/blob/master/src/com/esotericsoftware/kryo/Kryo.java#L179[default serializers] provided by Kryo, there are https://github.com/magro/kryo-serializers[other libraries] to help you with that

* avoid unnecessary partitioning (ex. groupByKey vs. reduceByKey)

image::images/groupByKey.png[]

image::images/reduceByKey.png[]

https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html[image source]

* check CPU hotspots with a profiler

image::images/CPU_HotSpot.png[CPU Hotspots]

* make sure your partitions are balanced (data is evenly distributed)

* prefer Async collection operation (i.e. collectAsync) for CPU related issues

* Combine multiple RDD.maps/flatMaps/filters into one single RDD.map operation if possible

== OutOfMemory issues

=== Symptoms 
* Explicit :
----
java.lang.OutOfMemoryError : GC overhead limit exceeded
----
or 
----
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000654a5e000, 837603328, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 837603328 bytes for committing reserved memory.
----
or 
----
Container killed by YARN for exceeding memory limits. 12.4 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
----

* Hidden
----
  TimeoutException
----
or 
----
20/08/17 14:09:23 WARN DataStreamer: Exception for BP-2134224599-192.168.152.25-1597671528722:blk_1073741843_1019
java.io.EOFException: Unexpected EOF while trying to read response from server
    at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:402)
    at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
    at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1073)
----

=== Solutions
* check yout GC logs 
in EMR env they are in stdout. 
----
2020-09-29T07:16:17.239+0000: [GC (Allocation Failure) 2020-09-29T07:16:17.239+0000: [ParNew: 629120K->30791K(629120K), 0.2974821 secs] 633015K->98181K(2027264K), 0.2975878 secs] [Times: user=3.71 sys=0.06, real=0.30 secs] 
2020-09-29T07:16:20.218+0000: [GC (Allocation Failure) 2020-09-29T07:16:20.218+0000: [ParNew: 590023K->69888K(629120K), 0.2883358 secs] 657413K->195919K(2027264K), 0.2884263 secs] [Times: user=0.71 sys=0.04, real=0.29 secs] 
2020-09-29T07:16:21.062+0000: [GC (Allocation Failure) 2020-09-29T07:16:21.062+0000: [ParNew: 629120K->69888K(629120K), 1.2208962 secs] 755151K->592684K(2027264K), 1.2209997 secs] [Times: user=2.76 sys=0.39, real=1.22 secs] 
2020-09-29T07:16:22.283+0000: [GC (CMS Initial Mark) [1 CMS-initial-mark: 522796K(1398144K)] 601001K(2027264K), 0.0134823 secs] [Times: user=0.02 sys=0.00, real=0.01 secs]
----

If needed you can use https://gceasy.io/[GC Easy]

* avoid collecting too much data on driver

* unpersist broadcasted variables as soon as possible ( sync if hot )

* use mapPartitions with Iterators when possible

* in hot areas we should use primitives instead of java classes and FastUtil collections inastead of Java/Scala collections

* detect memory size of your structures
  Quote from spark docs: The best way to size the amount of memory consumption a dataset will require is to create an RDD, put it into cache, and look at the   “Storage” page in the web UI. To estimate the memory consumption of a particular object, use SizeEstimator’s estimate method.
    
* avoid as much as possible transfers from java to scala structures and viceversa

* avoid as much as possible heavy-on-memory documented rdd operations ( groupBy, co-group even join ) as they rely spark.CompactBuffer(s) which stores data in memory

* Decrease the fraction of memory reserved for caching, using spark.storage.memoryFraction. If you don't use cache() or persist in your code, this might as well be 0. It's default is 0.6

* If your job doesn't need much shuffle memory then set shuffle.memory.fraction to a lower value (this might cause your shuffles to spill to disk which can have catastrophic impact on speed).

* OOM during shuffles do the opposite i.e. set shuffle.memory.fraction to something large, like 0.8
 
* don't go over 32 GB / executor

* show how to detect memory used by some structures

* if no memory leaks then consider increasing the driver / executor heap and their overhead


== Disk / Networking issues
=== Symptoms

=== Solutions
* check the size of your persisted data ( spark ui )
* controll your shuffles 
* kryo when possible
* controll the size of hdfs blocks
* gzip spilled data to disk ( see spark configuration for gzip ) 
* make sure you don’t log too much ( i.e opportunities )


