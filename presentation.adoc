= Spark best practices / troubleshooting spark issues 

== CPU / runtime performance issues

=== Symptoms

=== Solutions
* Kryo serialization
* persist when re-using RDDs
* Combine multiple RDD.maps/flatMaps/filters into one single RDD.map operation if possible
* avoid unnecessary partitioning ( see aggregate vs. reduceByKey )
* check the DAG generated by every operation done on your RDD ( example toLocalIterator creates one spark job for every partition)
* check CPU hotspots with a profiler
* make sure your partitions are balanced (picture from sampler here)

== OutOfMemory issues

=== Symptoms 
* java.lang.OutOfMemoryError : GC overhead limit exceeded
* Container killed by YARN for exceeding memory limits. 12.4 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
* often hidden as TimeoutException of IOException

=== Solutions
* unpersist broadcasted variables as soon as possible ( sync if hot )
* use mapPartitions with Iterators when possible
* in hot areas we should use primitives instead of java classes and FastUtil collections inastead of Java/Scala collections
* detect memory size of your structures 
  Quote from spark docs: The best way to size the amount of memory consumption a dataset will require is to create an RDD, put it into cache, and look at the   “Storage” page in the web UI. To estimate the memory consumption of a particular object, use SizeEstimator’s estimate method.
    
 * avoid as much as possible transfers from java to scala structures and viceversa
 * avoid as much as possible heavy-on-memory documented rdd operations ( groupBy, co-group even join ) as they rely spark.CompactBuffer(s) which stores data in memory
 * Decrease the fraction of memory reserved for caching, using spark.storage.memoryFraction. If you don't use cache() or persist in your code, this might as well be 0. It's default is 0.6
 * If your job doesn't need much shuffle memory then set shuffle.memory.fraction to a lower value (this might cause your shuffles to spill to disk which can have catastrophic impact on speed). 
 * OOM during shuffles do the opposite i.e. set shuffle.memory.fraction to something large, like 0.8
 
 * don't go over 32 GB / executor
    
  * take control of your memory settings
      ** control your driver and executor heap
      ** be aware of your executor overhead and/or control it


== Disk / Networking issues
=== Symptoms

=== Solutions
* check the size of your persisted data ( spark ui )
* controll your shuffles 
* kryo when possible
* controll the size of hdfs blocks
* gzip spilled data to disk ( see spark configuration for gzip ) 
* make sure you don’t log too much ( i.e opportunities )


