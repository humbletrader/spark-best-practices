= Spark best practices / troubleshooting spark issues 

== CPU / runtime performance issues

=== Symptoms
* slow processing time, obviously 
* just a fraction of your processing power is used

=== Solutions
* human error ( unneeded processing ) 
* Kryo serialization
* persist when re-using RDDs
* Combine multiple RDD.maps/flatMaps/filters into one single RDD.map operation if possible
* avoid unnecessary partitioning ( see aggregate vs. reduceByKey )
* check the DAG generated by every operation done on your RDD ( example toLocalIterator creates one spark job for every partition)
* check CPU hotspots with a profiler 
  
  image::./CPU_HotSpot.png[]
  
* make sure your partitions are balanced (picture from sampler here)

== OutOfMemory issues

=== Symptoms 
* Explicit :
----
java.lang.OutOfMemoryError : GC overhead limit exceeded
----
or 
----
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000654a5e000, 837603328, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 837603328 bytes for committing reserved memory.
----
or 
----
Container killed by YARN for exceeding memory limits. 12.4 GB of 12 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
----

* Hidden
----
  TimeoutException
----
or 
----
20/08/17 14:09:23 WARN DataStreamer: Exception for BP-2134224599-192.168.152.25-1597671528722:blk_1073741843_1019
java.io.EOFException: Unexpected EOF while trying to read response from server
    at org.apache.hadoop.hdfs.protocolPB.PBHelperClient.vintPrefixed(PBHelperClient.java:402)
    at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:213)
    at org.apache.hadoop.hdfs.DataStreamer$ResponseProcessor.run(DataStreamer.java:1073)
----

=== Solutions
* check yout GC logs 
in EMR env they are in stdout. 
----
2020-09-29T07:16:17.239+0000: [GC (Allocation Failure) 2020-09-29T07:16:17.239+0000: [ParNew: 629120K->30791K(629120K), 0.2974821 secs] 633015K->98181K(2027264K), 0.2975878 secs] [Times: user=3.71 sys=0.06, real=0.30 secs] 
2020-09-29T07:16:20.218+0000: [GC (Allocation Failure) 2020-09-29T07:16:20.218+0000: [ParNew: 590023K->69888K(629120K), 0.2883358 secs] 657413K->195919K(2027264K), 0.2884263 secs] [Times: user=0.71 sys=0.04, real=0.29 secs] 
2020-09-29T07:16:21.062+0000: [GC (Allocation Failure) 2020-09-29T07:16:21.062+0000: [ParNew: 629120K->69888K(629120K), 1.2208962 secs] 755151K->592684K(2027264K), 1.2209997 secs] [Times: user=2.76 sys=0.39, real=1.22 secs] 
2020-09-29T07:16:22.283+0000: [GC (CMS Initial Mark) [1 CMS-initial-mark: 522796K(1398144K)] 601001K(2027264K), 0.0134823 secs] [Times: user=0.02 sys=0.00, real=0.01 secs]
----

If needed you can use https://gceasy.io/

* unpersist broadcasted variables as soon as possible ( sync if hot )
* use mapPartitions with Iterators when possible
* in hot areas we should use primitives instead of java classes and FastUtil collections inastead of Java/Scala collections
* detect memory size of your structures 
  Quote from spark docs: The best way to size the amount of memory consumption a dataset will require is to create an RDD, put it into cache, and look at the   “Storage” page in the web UI. To estimate the memory consumption of a particular object, use SizeEstimator’s estimate method.
    
 * avoid as much as possible transfers from java to scala structures and viceversa
 * avoid as much as possible heavy-on-memory documented rdd operations ( groupBy, co-group even join ) as they rely spark.CompactBuffer(s) which stores data in memory
 * Decrease the fraction of memory reserved for caching, using spark.storage.memoryFraction. If you don't use cache() or persist in your code, this might as well be 0. It's default is 0.6
 * If your job doesn't need much shuffle memory then set shuffle.memory.fraction to a lower value (this might cause your shuffles to spill to disk which can have catastrophic impact on speed). 
 * OOM during shuffles do the opposite i.e. set shuffle.memory.fraction to something large, like 0.8
 
 * don't go over 32 GB / executor
    
 * take control of your memory settings
      ** control your driver and executor heap
      ** be aware of your executor overhead and/or control it
      
 * demo (show how to detect memory used by some structures)


== Disk / Networking issues
=== Symptoms

=== Solutions
* check the size of your persisted data ( spark ui )
* controll your shuffles 
* kryo when possible
* controll the size of hdfs blocks
* gzip spilled data to disk ( see spark configuration for gzip ) 
* make sure you don’t log too much ( i.e opportunities )


